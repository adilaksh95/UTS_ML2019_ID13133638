{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_13133638.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adilaksh95/UTS_ML2019_ID13133638/blob/master/A1_13133638.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vje0EyOKYFJ0",
        "colab_type": "text"
      },
      "source": [
        "Gitgub URL https://github.com/adilaksh95/UTS_ML2019_ID13133638/blob/master/A1_13133638.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX-Bcywrppx8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtFTXq0Xpsmw",
        "colab_type": "text"
      },
      "source": [
        "Literature review on Support Vector Networks by Corrina Cortes and Vladimir Vapnik\n",
        "\n",
        "# Introduction\n",
        "\n",
        "The support vector machine is a learning algorithm that uses binary classification to model the training data and then group new test data to either of the two groups created by the model. This is a linear classification, and there is a clear maximal margin dividing the two groups. For non-linear classification SVM's use a method called kernel trick, where the inputs are converted to high dimensional feature space and then a hyperplane constructed in that space.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ObEaID1zTNm",
        "colab_type": "text"
      },
      "source": [
        "# Content\n",
        "\n",
        "The paper talks about a new learning machine called support vector network, which can be used extensively for problems with two group classification. Further, it introduces a new idea of developing a non-linear solution for its training data that cannot be separated without errors in a linear hyperplane.\n",
        " \n",
        "\n",
        "The authors talk about three issues faced during learning. The first is finding the optimal hyperplane. For training data with precise bounds and separation, they can be linearly separated. The other case would be for non-separable data, and to do the same linear separation without errors is very difficult; hence, it becomes a quadratic programming problem. The authors further talk about escalation of the problem when the method fails to find a hyperplane of separation. This leads to constructing an incremental solution vector.\n",
        " \n",
        "\n",
        "In the case where the training dataset has vectors/points that cannot be separated without errors, we introduce a method called soft margin hyperplane. As such, in this scenario, for the sake of an efficient classification model, minimal error separation has to be considered. This construction has been approached in this paper by finding the solution of the dual quadratic programming problem.\n",
        " \n",
        "\n",
        "Once the data has been separated at the input stage, the data then has to be converted to a high dimensional space. As the stepwise problems in the input space are resolved, the finality is reached as the hyperplanes are constructed for the feature space. To this effect, a method of the convoluting the dot product for the feature space is used.\n",
        "\n",
        "The authors try to tackle the problem of creating the maximal margin hyperplane and represent it in the feature space through this paper and compare it with other classical algorithms.        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqyZYqt-zS43",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzNfE-sN6CS0",
        "colab_type": "text"
      },
      "source": [
        "# Innovation\n",
        "\n",
        "Previously, the data had a distinct separation point where the training vectors belonged to either of the given two quadrants. This separation was absolute and hence linear in the plane with an optimal maximal margin.\n",
        "The paper here introduces a new concept that builds on the former approach. This new method implements the separation of vectors in a non-linear pattern and also for data which cannot be separated linearly without errors occurring.\n",
        " This makes the support vector network elevate into a new class or method on par with neural networks.\n",
        " \n",
        "There are two cases in this separation scenario; the first is when the data points can be separated at the input plane without any errors  \n",
        " \n",
        "Two cases are considered before they implement the non-linear separation in the feature plane. The first is the input plane separation of data and in which no errors occur or are found. The other is where we find data to be impossible to separate without causing errors. Considering that case in a mathematical domain:\n",
        " \n",
        "For non-separable data where separation is not possible without error, we try to minimize the errors first by introducing non-negative variables in the equation below:\n",
        " \n",
        "$\\Phi (\\varepsilon )= \\sum_{i=1}^{l}\\varepsilon_{i}^{\\sigma }      \\varepsilon_{i}\\geq 0,i=1,....l$\n",
        " \n",
        "It is found that for sufficiently small $\\sigma$, the equation gives us the number of training errors. Minimizing and being rid of these small errors will help us separate the remaining training data without any errors. The separation is explained by the creation of the optimal hyperplane given by:\n",
        "       1/2W^2+CF$(\\sum_{i=1}^{l}\\varepsilon_{i}^{\\sigma })$\n",
        " \n",
        "The equation explains that with large constant values C and small \\sigma values, we can create a maximal separation for the training vectors while also reducing errors during separation.\n",
        "The opposite can be achieved with small constant C values and large \\sigma values.\n",
        " \n",
        "The soft margin introduction helps classify data that cannot be separated outright in a linear space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNaLPfgIH5fJ",
        "colab_type": "text"
      },
      "source": [
        "# Technical Quality\n",
        "\n",
        "This paper deals with two types of experimental analysis. First one involved creating an artificial set of patterns, and the other consists of the use of real-time data for digit recognition problem.\n",
        " \n",
        "The artificial patterns are a comparison of two different sets with the same parameter of a 2nd-degree polynomial; this showed that the 2nd degree did not make fewer errors in separation.\n",
        " \n",
        "Again, in real-time testing, two sets of data were used. A small database with 7300 training and 200 test patterns, and the larger one with 60,000 and 10,000 training and testing patterns respectively. The benchmark test comparison was done through experiments and examining other research papers.\n",
        " \n",
        "The raw error comparison of other classifiers and the n-degree polynomial of SVN/M shows that the latter has better accuracy with error peaking at 4.2% for the 6th-degree polynomial. Other than human performance at 2.5% error, algorithms that had less error rate was a 5-layer network (neural network) with a 5.1% error. Further performance improvement suggested for SVM involves the use of functions for dot product that reflects the problem at hand.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuJMb8FsTBXD",
        "colab_type": "text"
      },
      "source": [
        "#Application and X-factor\n",
        "\n"
      ]
    }
  ]
}